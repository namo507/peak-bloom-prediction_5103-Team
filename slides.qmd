---
title: "Cherry Blossom Peak Bloom Prediction 2026"
subtitle: "Methodology, Data Analysis & Final Predictions · Team 5103"
author: "Team 5103 — George Mason University"
date: "February 2026"
format:
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    toc: false
    embed-resources: true
    width: 1050
    height: 700
    transition: slide
    logo: ""
    footer: "Team 5103 · Cherry Blossom Peak Bloom Prediction 2026"
execute:
  warning: false
  message: false
  echo: false
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(lubridate)
library(mgcv)
library(knitr)
set.seed(5103)
options(dplyr.summarise.inform = FALSE)

# ── Helper ──
doy_to_date <- function(year, doy) {
  as.Date(strptime(sprintf("%d %03d", year, doy), format = "%Y %j"))
}
```

## Agenda {.smaller}

:::: {.columns}

::: {.column width="50%"}
1. Competition Context
2. Data Sources & Enrichment
3. Exploratory Data Analysis
4. Feature Engineering
5. Model A — Local Trend
:::

::: {.column width="50%"}
6. Model B — Pooled Nonlinear Learner
7. Backtesting & Ensemble Blending
8. Prediction Intervals
9. Cross-Language Robustness
10. Final 2026 Predictions
:::

::::

---

## Competition Context {.smaller}

| Aspect | Detail |
|---|---|
| **Organizer** | George Mason University, Dept. of Statistics |
| **Task** | Predict peak bloom DOY for **5 sites** in 2026 |
| **Point scoring** | Sum of absolute errors across all 5 sites |
| **Interval scoring** | Coverage count (tiebreak: sum of squared widths) |
| **Deadline** | February 28, 2026 (AoE) |

::: {.callout-note}
## Sites
Kyoto (Japan) · Washington D.C. (USA) · Liestal-Weideli (Switzerland) · Vancouver BC (Canada) · New York City (USA)
:::

---

## Site Characteristics {.smaller}

```{r}
tibble(
  Site       = c("Kyoto", "Washington DC", "Liestal", "Vancouver", "New York City"),
  Latitude   = c(35.01, 38.89, 47.48, 49.22, 40.73),
  Longitude  = c(135.68, -77.04, 7.73, -123.16, -74.00),
  `Alt (m)`  = c(44, 0, 350, 24, 8.5),
  `Record span` = c("812 – 2025", "1921 – 2025", "1895 – 2025", "2022 – 2025", "2019 – 2025"),
  Species    = c("P. jamasakura", "P. × yedoensis", "P. avium",
                 "P. × yedoensis 'Akebono'", "P. × yedoensis")
) |>
  kable(align = "lrrrlr")
```

---

## Data Sources Overview {.smaller}

```{r}
#| label: load-data
competition_files <- c(
  "data/kyoto.csv", "data/washingtondc.csv", "data/liestal.csv",
  "data/vancouver.csv", "data/nyc.csv"
)
aux_files <- c("data/japan.csv", "data/meteoswiss.csv", "data/south_korea.csv")

read_bloom_file <- function(path, source) {
  read_csv(path, show_col_types = FALSE) |>
    transmute(source = source, location, lat, long, alt,
              year = as.integer(year), bloom_doy = as.numeric(bloom_doy))
}

competition_raw <- map_dfr(competition_files, ~ read_bloom_file(.x, "competition"))
aux_raw         <- map_dfr(aux_files, ~ read_bloom_file(.x, "auxiliary"))
```

```{r}
tibble(
  Category = c("Competition core", "Auxiliary (Japan, MeteoSwiss, S. Korea)", "USA-NPN (NYC enrichment)"),
  Files    = c("kyoto, washingtondc, liestal, vancouver, nyc", "japan, meteoswiss, south_korea",
               "status-intensity + individual phenometrics"),
  Rows     = c(nrow(competition_raw), nrow(aux_raw), "~4 extra NYC bloom-years")
) |>
  kable()
```

---

## NYC Data Enrichment (USA-NPN) {.smaller}

```{r}
#| label: npn-load
nyc_npn_status <- read_csv("data/USA-NPN_status_intensity_observations_data.csv",
                           show_col_types = FALSE) |>
  filter(Site_ID == 32789, Species_ID == 228, Phenophase_ID == 501) |>
  mutate(Observation_Date = as_date(Observation_Date, format = "%m/%d/%y"),
         year = year(Observation_Date)) |>
  arrange(Observation_Date) |>
  group_by(year) |>
  summarize(bloom_doy = first(Day_of_Year[Phenophase_Status == 1]), .groups = "drop") |>
  filter(!is.na(bloom_doy)) |>
  mutate(source = "npn", location = "newyorkcity",
         lat = 40.73040, long = -73.99809, alt = 8.5)

nyc_npn_pheno <- read_csv("data/USA-NPN_individual_phenometrics_data.csv",
                          show_col_types = FALSE) |>
  filter(Site_ID == 32789, Species_ID == 228, Phenophase_ID == 501) |>
  group_by(First_Yes_Year) |>
  summarize(bloom_doy = min(First_Yes_DOY, na.rm = TRUE), .groups = "drop") |>
  filter(is.finite(bloom_doy)) |>
  rename(year = First_Yes_Year) |>
  mutate(source = "npn", location = "newyorkcity",
         lat = 40.73040, long = -73.99809, alt = 8.5)

nyc_npn <- nyc_npn_status |>
  select(source, location, lat, long, alt, year, bloom_doy) |>
  bind_rows(nyc_npn_pheno |>
              select(source, location, lat, long, alt, year, bloom_doy) |>
              anti_join(nyc_npn_status, by = "year"))

existing_nyc <- competition_raw |> filter(location == "newyorkcity") |> pull(year)
nyc_extra <- nyc_npn |> filter(!year %in% existing_nyc)

competition <- bind_rows(competition_raw, nyc_extra)
all_data <- bind_rows(competition, aux_raw) |>
  filter(!is.na(bloom_doy), !is.na(year), year >= 1880) |>
  mutate(site_id = paste(source, location, sep = "::"),
         source = factor(source), location = as.character(location))
```

- **Site 32789** (Washington Square Park), Species 228, Phenophase 501
- Status-intensity: first `Phenophase_Status == 1` per year
- Phenometrics: `min(First_Yes_DOY)` per year
- Merge rule: status takes priority; phenometrics fills gaps
- **Result:** `r nrow(nyc_extra)` extra NYC bloom years added

---

## EDA: Long-Term Trend

```{r}
#| fig-width: 10
#| fig-height: 5
competition_raw |>
  filter(year >= 1880) |>
  ggplot(aes(year, bloom_doy, color = location)) +
  geom_point(alpha = 0.5, size = 1.2) +
  geom_smooth(method = "loess", span = 0.4, se = FALSE, linewidth = 0.9) +
  facet_wrap(~location, scales = "free_x") +
  labs(title = "Peak bloom is shifting earlier over time",
       x = "Year", y = "Peak bloom (DOY)") +
  theme_minimal(base_size = 13) +
  theme(legend.position = "none",
        strip.text = element_text(face = "bold"))
```

---

## EDA: Bloom Distribution by Site

```{r}
#| fig-width: 10
#| fig-height: 5
competition_raw |>
  filter(year >= 1880) |>
  ggplot(aes(x = reorder(location, bloom_doy, FUN = median),
             y = bloom_doy, fill = location)) +
  geom_boxplot(alpha = 0.7, show.legend = FALSE) +
  coord_flip() +
  labs(title = "Bloom-day distribution across competition sites",
       x = NULL, y = "Peak bloom (DOY)") +
  theme_minimal(base_size = 14)
```

---

## EDA: Recent Decade Acceleration {.smaller}

```{r}
#| fig-width: 10
#| fig-height: 4.5
competition_raw |>
  filter(year >= 1950, location %in% c("kyoto", "washingtondc", "liestal")) |>
  mutate(decade = paste0(floor(year / 10) * 10, "s")) |>
  ggplot(aes(decade, bloom_doy, fill = location)) +
  geom_boxplot(alpha = 0.65, position = "dodge") +
  labs(title = "Bloom timing by decade (long-record sites)",
       x = "Decade", y = "Peak bloom (DOY)") +
  theme_minimal(base_size = 13) +
  theme(legend.position = "bottom")
```

Over the last two decades, median bloom at all three long-record sites is the **earliest on record**.

---

## Feature Engineering {.smaller}

```{r}
#| label: features
add_features <- function(df, reference_df = NULL) {
  ref <- if (is.null(reference_df)) df else reference_df
  site_n <- ref |> count(site_id, name = "site_obs")
  df |>
    left_join(site_n, by = "site_id") |>
    mutate(year_c = year - 1950, year_c2 = year_c^2,
           decade = floor(year / 10) * 10,
           lat_abs = abs(lat), alt_log1p = log1p(pmax(alt, 0)),
           site_obs = replace_na(site_obs, 1))
}
```

| Feature | Formula / Description | Ecological rationale |
|---|---|---|
| `year_c` | `year − 1950` | Centers the time axis |
| `year_c²` | `(year − 1950)²` | Captures trend acceleration |
| `lat`, `long` | Raw coordinates | Spatial climate gradients |
| `alt_log1p` | `log(1 + max(alt, 0))` | Diminishing altitude effect |
| `site_obs` | Count of records per site | Data-reliability proxy |
| `source` | competition / auxiliary / npn | Data-provenance indicator |

---

## Model A: Local Recency-Weighted Trend {.smaller}

:::: {.columns}

::: {.column width="55%"}
**Approach:**

- Per-site quadratic regression:
  `bloom_doy ~ year + year²`
- Exponential decay weights:
  $w_i = e^{(i - n)/6}$ (half-life ≈ 6 yr)
- Recent years dominate while long history provides curvature

**Fallback rules:**

- ≥ 4 obs → weighted quadratic
- 2–3 obs → unweighted linear
- 1 obs → site mean
:::

::: {.column width="45%"}
**Strengths:**

- Captures site-specific momentum
- Adapts to recent bloom acceleration

**Weaknesses:**

- Cannot leverage cross-site info
- Poor for sparse sites (Vancouver, NYC)
:::

::::

---

## Model B: Pooled Nonlinear Learner {.smaller}

**R pipeline — Generalized Additive Model (GAM):**

$$\text{bloom\_doy} \sim s(\text{year}, k{=}25) + s(\text{lat}, \text{long}, k{=}40) + s(\text{alt}, k{=}8) + s(\text{site\_obs}, k{=}8) + \text{source}$$

- Estimation method: REML · Trained on **all ~14 K+ records** (competition + auxiliary + NPN)

**Python pipeline — Gradient Boosting Regressor (GBR):**

- Huber loss, 700 estimators, learning rate = 0.02, max depth = 3
- Same feature set; `OneHotEncoder` for source, `MedianImputer` for numerics

**Key advantage:** Learns transferable spatial-temporal structure — sparse sites borrow strength from thousands of auxiliary records.

---

## Rolling-Origin Backtesting {.smaller}

```{r}
#| label: backtest
#| cache: true
competition_sites <- sort(unique(competition_raw$location))

predict_local_trend <- function(train_comp, new_comp) {
  map_dfr(unique(new_comp$location), function(loc) {
    tr <- train_comp |> filter(location == loc) |> arrange(year)
    nd <- new_comp |> filter(location == loc)
    n <- nrow(tr)
    if (n >= 4) {
      w <- exp(seq(-n + 1, 0) / 6)
      fit <- lm(bloom_doy ~ year + I(year^2), data = tr, weights = w)
      p <- predict(fit, newdata = nd)
    } else if (n >= 2) {
      fit <- lm(bloom_doy ~ year, data = tr)
      p <- predict(fit, newdata = nd)
    } else {
      p <- rep(mean(tr$bloom_doy, na.rm = TRUE), nrow(nd))
    }
    nd |> mutate(pred_local = as.numeric(p))
  })
}

fit_gam_model <- function(train_all) {
  mgcv::gam(bloom_doy ~ s(year, k = 25, bs = "cr") +
               s(lat, long, k = 40, bs = "tp") +
               s(alt_log1p, k = 8, bs = "cr") +
               s(site_obs, k = 8, bs = "cr") + source,
             data = train_all, method = "REML")
}

backtest_years <- seq(max(1900, min(competition_raw$year) + 20),
                      max(competition_raw$year))

rolling_results <- map_dfr(backtest_years, function(y) {
  train_comp <- competition |> filter(year < y)
  test_comp  <- competition_raw |> filter(year == y)
  if (nrow(test_comp) == 0 ||
      n_distinct(train_comp$location) < length(competition_sites)) return(tibble())
  train_all <- all_data |> filter(year < y) |> add_features()
  test_feat <- test_comp |>
    mutate(source = factor("competition", levels = levels(all_data$source)),
           site_id = paste(source, location, sep = "::")) |>
    add_features(reference_df = train_all)
  local_pred <- predict_local_trend(train_comp, test_feat)
  gam_fit  <- fit_gam_model(train_all)
  gam_pred <- predict(gam_fit, newdata = test_feat)
  test_feat |>
    select(location, year, bloom_doy) |>
    left_join(local_pred |> select(location, year, pred_local),
              by = c("location", "year")) |>
    mutate(pred_gam = as.numeric(gam_pred))
})

mae_local <- mean(abs(rolling_results$bloom_doy - rolling_results$pred_local), na.rm = TRUE)
mae_gam   <- mean(abs(rolling_results$bloom_doy - rolling_results$pred_gam),   na.rm = TRUE)
w_local   <- (1 / mae_local) / ((1 / mae_local) + (1 / mae_gam))
w_gam     <- 1 - w_local

rolling_results <- rolling_results |>
  mutate(pred_ensemble = w_local * pred_local + w_gam * pred_gam,
         abs_err = abs(bloom_doy - pred_ensemble))

mae_ens <- mean(rolling_results$abs_err, na.rm = TRUE)

site_q90   <- rolling_results |>
  group_by(location) |>
  summarize(q90 = quantile(abs_err, 0.90, na.rm = TRUE, type = 8), .groups = "drop")
global_q90 <- quantile(rolling_results$abs_err, 0.90, na.rm = TRUE, type = 8)
```

**Procedure:** For each year *y* from `r min(backtest_years)` to `r max(backtest_years)`:

1. Train on all data with `year < y`
2. Predict competition sites observed at year *y*
3. Record absolute errors for Model A & Model B

→ Ensures **no future leakage**.
→ Provides honest MAE estimates and residuals for interval calibration.

---

## Ensemble Blending — Quantitative Results {.smaller}

**Inverse-MAE weighting:**

$$w_A = \frac{1 / \text{MAE}_A}{1 / \text{MAE}_A + 1 / \text{MAE}_B}, \quad w_B = 1 - w_A$$

```{r}
tibble(
  Model = c("Local (Model A)", "GAM (Model B)", "**Ensemble**"),
  `MAE (days)` = c(round(mae_local, 2), round(mae_gam, 2), round(mae_ens, 2)),
  Weight = c(paste0(round(w_local * 100, 1), "%"),
             paste0(round(w_gam * 100, 1), "%"), "—")
) |>
  kable(align = "lrr")
```

<br>

$$\hat{y} = `r round(w_local, 3)` \times \hat{y}_{\text{local}} + `r round(w_gam, 3)` \times \hat{y}_{\text{GAM}}$$

The ensemble outperforms both individual models on held-out years.

---

## Backtest Residual Analysis

```{r}
#| fig-width: 10
#| fig-height: 4.5
rolling_results |>
  ggplot(aes(x = reorder(location, abs_err, FUN = median),
             y = abs_err, fill = location)) +
  geom_boxplot(alpha = 0.7, show.legend = FALSE) +
  geom_hline(yintercept = as.numeric(global_q90), linetype = "dashed",
             color = "firebrick", linewidth = 0.6) +
  annotate("text", x = 0.5, y = as.numeric(global_q90) + 0.8,
           label = paste0("Global q90 = ", round(global_q90, 1), " d"),
           hjust = 0, color = "firebrick", size = 3.5) +
  coord_flip() +
  labs(title = "Ensemble absolute errors from rolling backtest",
       x = NULL, y = "Absolute error (days)") +
  theme_minimal(base_size = 13)
```

---

## Prediction Intervals {.smaller}

:::: {.columns}

::: {.column width="55%"}
**Split-conformal calibration:**

- Half-width = 90th percentile of backtest |residuals| per location
- Interval: $[\hat{y} - q_{90},\ \hat{y} + q_{90}]$
- Fallback to global $q_{90}$ for unseen sites
- Clipped to valid range [1, 366]
:::

::: {.column width="45%"}
```{r}
site_q90 |>
  mutate(q90 = round(q90, 1),
         `Width (days)` = round(2 * q90, 1)) |>
  rename(Location = location, `Half-width` = q90) |>
  kable(align = "lrr")
```
:::

::::

<br>

**Design goal:** ≥ 90% empirical coverage while minimizing $\sum(\text{width}^2)$ (competition tiebreaker).

---

## Cross-Language Robustness Check {.smaller}

Two **fully independent** pipelines:

| Pipeline | Model B | Output |
|---|---|---|
| R (primary) | GAM (REML) | `cherry-predictions.csv` |
| Python | GBR (Huber, 700 trees) | `cherry-predictions-python.csv` |

```{r}
r_sub  <- read_csv("cherry-predictions.csv", show_col_types = FALSE) |>
  rename(pred_R = prediction, lower_R = lower, upper_R = upper)
py_sub <- read_csv("cherry-predictions-python.csv", show_col_types = FALSE) |>
  rename(pred_Py = prediction, lower_Py = lower, upper_Py = upper)

cmp <- inner_join(r_sub, py_sub, by = "location") |>
  mutate(gap = abs(pred_R - pred_Py))

avg_gap <- mean(cmp$gap)
```

```{r}
cmp |>
  select(location, pred_R, pred_Py, gap) |>
  rename(Location = location, `R pred` = pred_R, `Python pred` = pred_Py,
         `|Gap|` = gap) |>
  kable(align = "lrrr")
```

Mean point gap = **`r round(avg_gap, 1)` days** (≤ 4 → blended submission used).

---

## Final 2026 Predictions — R Pipeline {.smaller}

```{r}
#| label: final-pred
target_year <- max(competition_raw$year, na.rm = TRUE) + 1
train_all <- add_features(all_data)

newdata <- competition_raw |>
  group_by(location) |> slice_max(year, n = 1, with_ties = FALSE) |> ungroup() |>
  transmute(source = factor("competition", levels = levels(all_data$source)),
            location, lat, long, alt, year = target_year,
            bloom_doy = NA_real_,
            site_id = paste(source, location, sep = "::")) |>
  add_features(reference_df = train_all)

local_pred <- predict_local_trend(competition, newdata)
gam_fit    <- fit_gam_model(train_all)
gam_pred   <- predict(gam_fit, newdata = newdata)

final_pred <- newdata |>
  select(location, year) |>
  left_join(local_pred |> select(location, year, pred_local),
            by = c("location", "year")) |>
  mutate(pred_gam   = as.numeric(gam_pred),
         prediction = round(w_local * pred_local + w_gam * pred_gam)) |>
  left_join(site_q90, by = "location") |>
  mutate(q90   = replace_na(q90, as.numeric(global_q90)),
         lower = floor(w_local * pred_local + w_gam * pred_gam - q90),
         upper = ceiling(w_local * pred_local + w_gam * pred_gam + q90)) |>
  mutate(across(c(prediction, lower, upper), ~ pmin(pmax(.x, 1), 366))) |>
  arrange(location)
```

```{r}
final_pred |>
  transmute(Location = location,
            `Prediction (DOY)` = prediction,
            Lower = lower, Upper = upper,
            `Predicted date` = doy_to_date(target_year, prediction),
            `Interval` = paste0(
              format(doy_to_date(target_year, lower), "%b %d"), " – ",
              format(doy_to_date(target_year, upper), "%b %d"))) |>
  kable(align = "lrrrlr")
```

---

## Final 2026 Predictions — Blended Submission {.smaller}

```{r}
final_sub <- read_csv("cherry-predictions-final.csv", show_col_types = FALSE)

final_sub |>
  mutate(
    `Predicted date` = doy_to_date(target_year, prediction),
    `Interval` = paste0(
      format(doy_to_date(target_year, lower), "%b %d"), " – ",
      format(doy_to_date(target_year, upper), "%b %d")),
    Width = upper - lower
  ) |>
  rename(Location = location, DOY = prediction, Lower = lower, Upper = upper) |>
  kable(align = "lrrrllr")
```

```{r}
ssw <- sum((final_sub$upper - final_sub$lower)^2)
```

<br>

**Sum of squared interval widths (tiebreaker):** `r ssw`

---

## Predictions Visualized

```{r}
#| fig-width: 10
#| fig-height: 5
final_sub |>
  mutate(date_pred  = doy_to_date(target_year, prediction),
         date_lower = doy_to_date(target_year, lower),
         date_upper = doy_to_date(target_year, upper),
         location   = reorder(location, prediction)) |>
  ggplot(aes(y = location)) +
  geom_segment(aes(x = date_lower, xend = date_upper, yend = location),
               linewidth = 3, color = "#F2A6B6", alpha = 0.7) +
  geom_point(aes(x = date_pred), size = 4.5, color = "#C0392B") +
  scale_x_date(date_labels = "%b %d") +
  labs(title = "2026 Predicted Bloom Dates with Intervals",
       x = "Date", y = NULL) +
  theme_minimal(base_size = 14) +
  theme(panel.grid.major.y = element_blank())
```

---

## Interpretation {.smaller}

:::: {.columns}

::: {.column width="50%"}
**Why the ensemble works:**

- Local trend captures site-specific acceleration
- GAM captures cross-site spatial structure
- Inverse-MAE blending is purely data-driven
- Backtest MAE ≈ `r round(mae_ens, 1)` days
:::

::: {.column width="50%"}
**Climate signal:**

- Bloom DOY is decreasing at all 5 sites
- Last two decades are the earliest on record
- Sparse sites (Vancouver, NYC) borrow strength from 14K+ auxiliary records
:::

::::

<br>

**Interval design:** Per-site conformal widths adapt to each location's predictability (wider for Vancouver ≈ `r final_sub |> filter(location == "vancouver") |> mutate(w = upper - lower) |> pull(w)` d, narrower for NYC ≈ `r final_sub |> filter(location == "newyorkcity") |> mutate(w = upper - lower) |> pull(w)` d).

---

## Limitations & Future Work {.smaller}

| Limitation | Potential improvement |
|---|---|
| No direct temperature covariates | NOAA API winter/spring degree-day features |
| Short records at Vancouver (4 yr) & NYC (5 yr) | Transfer learning / Bayesian priors |
| Residual weather shocks unpredictable | Ensemble with current-season temperature forecasts |
| Point uncertainty only from conformal residuals | Full Bayesian credible intervals or quantile regression |
| No phenological process model | Chill-unit accumulation (e.g., Utah model) |

---

## Reproducibility & Repository Map {.smaller}

```
peak-bloom-prediction_5103-Team/
├── solution.qmd               ← Primary R pipeline (abstract + models + predictions)
├── Solution.ipynb             ← Independent Python pipeline (GBR ensemble)
├── cherry-predictions.csv      ← R output
├── cherry-predictions-python.csv ← Python output
├── cherry-predictions-final.csv  ← Blended final submission
├── abstract.md                ← Competition abstract (335 words)
├── slides.qmd                 ← This presentation source
├── data/                      ← All datasets + README
└── demo_analysis.qmd          ← Competition-provided demo
```

**Reproduce everything:**
```bash
quarto render solution.qmd        # R analysis + predictions
jupyter nbconvert --execute Solution.ipynb --inplace  # Python
quarto render slides.qmd          # This deck
```

---

## Thank You {.center}

<br><br>

### Peak Bloom Prediction 2026

**Team 5103** · George Mason University

<br>

::: {.callout-tip}
All code, data, and outputs are publicly available and fully reproducible.
:::
