---
title: "Cherry Blossom Peak Bloom Prediction (Team 5103)"
subtitle: "Methodology, data analysis, and 2026 predictions"
author: "Team 5103"
format:
  revealjs:
    theme: simple
    slide-number: true
    toc: true
    embed-resources: true
execute:
  warning: false
  message: false
---

## Setup

```{r}
library(tidyverse)
library(lubridate)
```

## Competition context

- Predict next-year peak bloom day-of-year (DOY) for 5 sites.
- Evaluation: point accuracy (absolute error) + interval quality (coverage, then width).
- Full rules and framing are in `README.md`.

## What this project does

- Reproducible R workflow in `solution.qmd`
- Independent Python check in `Solution.ipynb`
- Final outputs:
  - `cherry-predictions.csv`
  - `cherry-predictions-python.csv`
  - `cherry-predictions-final.csv`

## Data used

- Core competition data: Kyoto, Washington DC, Liestal, Vancouver, NYC
- Auxiliary data: Japan regional, MeteoSwiss, South Korea, USA-NPN (NYC)
- Data structure and licensing notes: `data/README.md`

## Load core data

```{r}
competition_files <- c(
  "data/kyoto.csv",
  "data/washingtondc.csv",
  "data/liestal.csv",
  "data/vancouver.csv",
  "data/nyc.csv"
)

read_bloom_file <- function(path, source = "competition") {
  read_csv(path, show_col_types = FALSE) |>
    transmute(
      source = source,
      location,
      lat,
      long,
      alt,
      year = as.integer(year),
      bloom_doy = as.numeric(bloom_doy)
    )
}

competition_raw <- map_dfr(competition_files, read_bloom_file)
competition_raw
```

## Exploratory data analysis: long-term trend

```{r}
competition_raw |>
  filter(year >= 1880) |>
  ggplot(aes(year, bloom_doy, color = location)) +
  geom_point(alpha = 0.6, size = 1) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.8) +
  facet_wrap(~location, scales = "free_x") +
  labs(
    title = "Peak bloom tends to shift earlier over time",
    x = "Year", y = "Peak bloom (DOY)"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")
```

## Data enrichment for NYC (USA-NPN)

```{r}
npn_status <- read_csv("data/USA-NPN_status_intensity_observations_data.csv", show_col_types = FALSE) |>
  filter(Site_ID == 32789, Species_ID == 228, Phenophase_ID == 501) |>
  mutate(
    Observation_Date = as_date(Observation_Date, format = "%m/%d/%y"),
    year = year(Observation_Date)
  ) |>
  arrange(Observation_Date) |>
  group_by(year) |>
  summarize(
    bloom_doy = first(Day_of_Year[Phenophase_Status == 1]),
    .groups = "drop"
  ) |>
  filter(!is.na(bloom_doy))

npn_status
```

## Features used in modeling

- Time: year, centered year, squared year
- Geography: latitude, longitude, altitude (log-transformed)
- Data reliability proxy: site observation count
- Source indicator (competition vs auxiliary vs NPN)

(Implemented in `add_features` in `solution.qmd`.)

## Model A: local trend

- Site-wise recency-weighted quadratic (fallback to linear/mean when sparse)
- Captures local momentum and curvature
- Implemented in `predict_local_trend` in `solution.qmd`

## Model B: pooled nonlinear model

- R pipeline: GAM with smooths over year, spatial terms, altitude, site depth
- Implemented in `fit_gam_model` in `solution.qmd`
- Python pipeline: Gradient Boosting Regressor in `Solution.ipynb`

## Backtesting and ensemble blending

- Rolling-origin backtest over historical years
- Compute MAE for local and pooled models
- Blend weights via inverse-MAE:
  - better out-of-sample model gets larger weight
- Implemented in rolling section of `solution.qmd`

## Prediction intervals

- Split-conformal style calibration
- 90th percentile of absolute residuals by location
- Fallback to global residual quantile when needed

## Final predictions (from file)

```{r}
pred_file <- if (file.exists("cherry-predictions-final.csv")) {
  "cherry-predictions-final.csv"
} else if (file.exists("cherry-predictions.csv")) {
  "cherry-predictions.csv"
} else {
  NA_character_
}

if (!is.na(pred_file)) {
  read_csv(pred_file, show_col_types = FALSE) |>
    arrange(location)
} else {
  tibble(note = "Run solution pipeline first to generate prediction CSV.")
}
```

## Interpretation

- Ensemble improves robustness by combining:
  1. local historical behavior
  2. cross-site transferable structure
- Intervals are calibrated to target coverage while controlling width
- Approach is reproducible and competition-aligned

## Limitations and next steps

- Residual weather shocks remain hard to predict
- Potential improvements:
  - engineered temperature covariates
  - richer uncertainty modeling
  - model stacking with strict out-of-sample validation

## Reproducibility

- Render slides:

```bash
quarto render slides.qmd
```

- Main artifacts:
  - `solution.qmd`
  - `Solution.ipynb`
  - `README.md`
  - `data/README.md`
