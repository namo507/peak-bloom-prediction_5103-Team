---
title: "Cherry Blossom Peak Bloom Prediction 2026"
subtitle: "Team 5103 — University of Maryland"
date: "February 2026"
format:
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    toc: false
    embed-resources: true
    width: 1050
    height: 700
    transition: slide
    footer: "Team 5103 · University of Maryland · Cherry Blossom Peak Bloom Prediction 2026"
execute:
  warning: false
  message: false
  echo: false
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(lubridate)
library(mgcv)
library(knitr)
set.seed(5103)
options(dplyr.summarise.inform = FALSE)

doy_to_date <- function(year, doy) {
  as.Date(strptime(sprintf("%d %03d", year, doy), format = "%Y %j"))
}

# ── Load all data ──
competition_files <- c("data/kyoto.csv", "data/washingtondc.csv",
                       "data/liestal.csv", "data/vancouver.csv", "data/nyc.csv")
aux_files <- c("data/japan.csv", "data/meteoswiss.csv", "data/south_korea.csv")

read_bloom_file <- function(path, source) {
  read_csv(path, show_col_types = FALSE) |>
    transmute(source = source, location, lat, long, alt,
              year = as.integer(year), bloom_doy = as.numeric(bloom_doy))
}

competition_raw <- map_dfr(competition_files, ~ read_bloom_file(.x, "competition"))
aux_raw         <- map_dfr(aux_files, ~ read_bloom_file(.x, "auxiliary"))

# NPN enrichment
nyc_npn_status <- read_csv("data/USA-NPN_status_intensity_observations_data.csv",
                           show_col_types = FALSE) |>
  filter(Site_ID == 32789, Species_ID == 228, Phenophase_ID == 501) |>
  mutate(Observation_Date = as_date(Observation_Date, format = "%m/%d/%y"),
         year = year(Observation_Date)) |>
  arrange(Observation_Date) |>
  group_by(year) |>
  summarize(bloom_doy = first(Day_of_Year[Phenophase_Status == 1]), .groups = "drop") |>
  filter(!is.na(bloom_doy)) |>
  mutate(source = "npn", location = "newyorkcity",
         lat = 40.73040, long = -73.99809, alt = 8.5) |>
  select(source, location, lat, long, alt, year, bloom_doy)

nyc_npn_pheno <- read_csv("data/USA-NPN_individual_phenometrics_data.csv",
                          show_col_types = FALSE) |>
  filter(Site_ID == 32789, Species_ID == 228, Phenophase_ID == 501) |>
  group_by(First_Yes_Year) |>
  summarize(bloom_doy = min(First_Yes_DOY, na.rm = TRUE), .groups = "drop") |>
  filter(is.finite(bloom_doy)) |>
  rename(year = First_Yes_Year) |>
  mutate(source = "npn", location = "newyorkcity",
         lat = 40.73040, long = -73.99809, alt = 8.5) |>
  select(source, location, lat, long, alt, year, bloom_doy)

nyc_npn <- bind_rows(nyc_npn_status,
                     nyc_npn_pheno |> anti_join(nyc_npn_status, by = "year"))
existing_nyc <- competition_raw |> filter(location == "newyorkcity") |> pull(year)
nyc_extra <- nyc_npn |> filter(!year %in% existing_nyc)

competition <- bind_rows(competition_raw, nyc_extra)
all_data <- bind_rows(competition, aux_raw) |>
  filter(!is.na(bloom_doy), !is.na(year), year >= 1880) |>
  mutate(site_id = paste(source, location, sep = "::"),
         source = factor(source), location = as.character(location))

competition_sites <- sort(unique(competition_raw$location))
target_year <- max(competition_raw$year, na.rm = TRUE) + 1

# ── Feature engineering ──
add_features <- function(df, reference_df = NULL) {
  ref <- if (is.null(reference_df)) df else reference_df
  site_n <- ref |> count(site_id, name = "site_obs")
  df |>
    left_join(site_n, by = "site_id") |>
    mutate(year_c = year - 1950, year_c2 = year_c^2,
           decade = floor(year / 10) * 10,
           lat_abs = abs(lat), alt_log1p = log1p(pmax(alt, 0)),
           site_obs = replace_na(site_obs, 1))
}

# ── Models ──
predict_local_trend <- function(train_comp, new_comp) {
  map_dfr(unique(new_comp$location), function(loc) {
    tr <- train_comp |> filter(location == loc) |> arrange(year)
    nd <- new_comp |> filter(location == loc)
    n <- nrow(tr)
    if (n >= 4) {
      w <- exp(seq(-n + 1, 0) / 6)
      fit <- lm(bloom_doy ~ year + I(year^2), data = tr, weights = w)
      p <- predict(fit, newdata = nd)
    } else if (n >= 2) {
      fit <- lm(bloom_doy ~ year, data = tr); p <- predict(fit, newdata = nd)
    } else { p <- rep(mean(tr$bloom_doy, na.rm = TRUE), nrow(nd)) }
    nd |> mutate(pred_local = as.numeric(p))
  })
}

fit_gam_model <- function(train_all) {
  mgcv::gam(bloom_doy ~ s(year, k = 25, bs = "cr") +
               s(lat, long, k = 40, bs = "tp") +
               s(alt_log1p, k = 8, bs = "cr") +
               s(site_obs, k = 8, bs = "cr") + source,
             data = train_all, method = "REML")
}

# ── Rolling backtest ──
backtest_years <- seq(max(1900, min(competition_raw$year) + 20),
                      max(competition_raw$year))

rolling_results <- map_dfr(backtest_years, function(y) {
  train_comp <- competition |> filter(year < y)
  test_comp  <- competition_raw |> filter(year == y)
  if (nrow(test_comp) == 0 ||
      n_distinct(train_comp$location) < length(competition_sites)) return(tibble())
  train_all <- all_data |> filter(year < y) |> add_features()
  test_feat <- test_comp |>
    mutate(source = factor("competition", levels = levels(all_data$source)),
           site_id = paste(source, location, sep = "::")) |>
    add_features(reference_df = train_all)
  local_pred <- predict_local_trend(train_comp, test_feat)
  gam_fit  <- fit_gam_model(train_all)
  gam_pred <- predict(gam_fit, newdata = test_feat)
  test_feat |>
    select(location, year, bloom_doy) |>
    left_join(local_pred |> select(location, year, pred_local),
              by = c("location", "year")) |>
    mutate(pred_gam = as.numeric(gam_pred))
})

mae_local <- mean(abs(rolling_results$bloom_doy - rolling_results$pred_local), na.rm = TRUE)
mae_gam   <- mean(abs(rolling_results$bloom_doy - rolling_results$pred_gam),   na.rm = TRUE)
w_local   <- (1 / mae_local) / ((1 / mae_local) + (1 / mae_gam))
w_gam     <- 1 - w_local

rolling_results <- rolling_results |>
  mutate(pred_ensemble = w_local * pred_local + w_gam * pred_gam,
         abs_err = abs(bloom_doy - pred_ensemble))
mae_ens <- mean(rolling_results$abs_err, na.rm = TRUE)

site_q90   <- rolling_results |>
  group_by(location) |>
  summarize(q90 = quantile(abs_err, 0.90, na.rm = TRUE, type = 8), .groups = "drop")
global_q90 <- quantile(rolling_results$abs_err, 0.90, na.rm = TRUE, type = 8)

# ── Final prediction ──
train_all <- add_features(all_data)
newdata <- competition_raw |>
  group_by(location) |> slice_max(year, n = 1, with_ties = FALSE) |> ungroup() |>
  transmute(source = factor("competition", levels = levels(all_data$source)),
            location, lat, long, alt, year = target_year,
            bloom_doy = NA_real_, site_id = paste(source, location, sep = "::")) |>
  add_features(reference_df = train_all)

local_pred <- predict_local_trend(competition, newdata)
gam_fit    <- fit_gam_model(train_all)
gam_pred   <- predict(gam_fit, newdata = newdata)

final_pred <- newdata |>
  select(location, year) |>
  left_join(local_pred |> select(location, year, pred_local), by = c("location", "year")) |>
  mutate(pred_gam = as.numeric(gam_pred),
         prediction = round(w_local * pred_local + w_gam * pred_gam)) |>
  left_join(site_q90, by = "location") |>
  mutate(q90 = replace_na(q90, as.numeric(global_q90)),
         lower = floor(w_local * pred_local + w_gam * pred_gam - q90),
         upper = ceiling(w_local * pred_local + w_gam * pred_gam + q90)) |>
  mutate(across(c(prediction, lower, upper), ~ pmin(pmax(.x, 1), 366))) |>
  arrange(location)

final_sub <- read_csv("cherry-predictions-final.csv", show_col_types = FALSE)
ssw <- sum((final_sub$upper - final_sub$lower)^2)

r_sub  <- read_csv("cherry-predictions.csv", show_col_types = FALSE) |>
  rename(pred_R = prediction)
py_sub <- read_csv("cherry-predictions-python.csv", show_col_types = FALSE) |>
  rename(pred_Py = prediction)
cmp <- inner_join(r_sub, py_sub |> select(location, pred_Py), by = "location") |>
  mutate(gap = abs(pred_R - pred_Py))
avg_gap <- mean(cmp$gap)
```

## Datasets Overview {.smaller}

:::: {.columns}

::: {.column width="55%"}

**Competition datasets** — bloom DOY for 5 target sites:

| Dataset | Location | Records | Span |
|---|---|---:|---|
| `kyoto.csv` | Kyoto, Japan | 837 | 812 – 2025 |
| `washingtondc.csv` | Washington DC | 106 | 1921 – 2025 |
| `liestal.csv` | Liestal, Switzerland | 132 | 1895 – 2025 |
| `vancouver.csv` | Vancouver, Canada | 4 | 2022 – 2025 |
| `nyc.csv` | New York City | 3 | 2019 – 2025 |

**Auxiliary datasets** — broaden geographic & temporal coverage:

| Dataset | Records |
|---|---:|
| `japan.csv` (regional bloom dates) | 6,573 |
| `meteoswiss.csv` (Swiss phenology) | 6,642 |
| `south_korea.csv` | 994 |

:::

::: {.column width="45%"}

**USA-NPN enrichment (NYC):**

- Site 32789 (Washington Square Park)
- Species 228 (*Prunus × yedoensis*)
- Phenophase 501 (Open flowers)
- `r nrow(nyc_extra)` extra bloom-year records added

**Total training pool:** ~`r format(nrow(all_data), big.mark = ",")` rows

```{r}
#| fig-width: 5
#| fig-height: 3.2
all_data |>
  count(source) |>
  ggplot(aes(reorder(source, -n), n, fill = source)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  geom_text(aes(label = format(n, big.mark = ",")), vjust = -0.3, size = 3.5) +
  labs(x = NULL, y = "Records") +
  theme_minimal(base_size = 11) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.15)))
```

:::

::::

---

## Models & Methodology {.smaller}

:::: {.columns}

::: {.column width="50%"}

**Model A — Local Trend** (per site)

- Recency-weighted quadratic: `bloom_doy ~ year + year²`
- Weights: $w_i = e^{(i-n)/6}$, half-life ≈ 6 yr
- Fallback: linear (2–3 obs) or mean (1 obs)
- Captures site-specific momentum

**Model B — Pooled GAM** (all sites jointly)

$$\text{DOY} \sim s(\text{year}) + s(\text{lat}, \text{long}) + s(\text{alt}) + s(\text{site\_obs}) + \text{source}$$

- REML estimation on **`r format(nrow(all_data), big.mark=",")`** records
- Python check: GBR (Huber, 700 trees, lr = 0.02)
:::

::: {.column width="50%"}

**Ensemble blending** (data-driven)

- Rolling-origin backtest (`r min(backtest_years)` – `r max(backtest_years)`)
- Inverse-MAE weights from out-of-sample errors:

$$w_A = \tfrac{1/\text{MAE}_A}{1/\text{MAE}_A + 1/\text{MAE}_B}$$

| Model | Backtest MAE |
|---|---:|
| Local (A) | `r round(mae_local, 2)` days |
| GAM (B) | `r round(mae_gam, 2)` days |
| **Ensemble** | **`r round(mae_ens, 2)` days** |

**Prediction intervals** — split-conformal: 90th-percentile of backtest |residuals| per location → half-width of interval

:::

::::

---

## Backtest Performance & EDA {.smaller}

:::: {.columns}

::: {.column width="50%"}

```{r}
#| fig-width: 5
#| fig-height: 4
competition_raw |>
  filter(year >= 1880) |>
  ggplot(aes(year, bloom_doy, color = location)) +
  geom_point(alpha = 0.4, size = 0.9) +
  geom_smooth(method = "loess", span = 0.4, se = FALSE, linewidth = 0.7) +
  facet_wrap(~location, scales = "free_x", ncol = 1) +
  labs(title = "Bloom shifts earlier over time",
       x = "Year", y = "DOY") +
  theme_minimal(base_size = 9) +
  theme(legend.position = "none", strip.text = element_text(face = "bold"))
```

:::

::: {.column width="50%"}

```{r}
#| fig-width: 5
#| fig-height: 4
rolling_results |>
  ggplot(aes(x = reorder(location, abs_err, FUN = median),
             y = abs_err, fill = location)) +
  geom_boxplot(alpha = 0.7, show.legend = FALSE) +
  geom_hline(yintercept = as.numeric(global_q90), linetype = "dashed",
             color = "firebrick", linewidth = 0.5) +
  annotate("text", x = 0.6, y = as.numeric(global_q90) + 0.6,
           label = paste0("q90 = ", round(global_q90, 1), " d"),
           hjust = 0, color = "firebrick", size = 3) +
  coord_flip() +
  labs(title = "Ensemble backtest errors by site",
       x = NULL, y = "Absolute error (days)") +
  theme_minimal(base_size = 9)
```

:::

::::

- All 5 sites show a **downward bloom-DOY trend** — climate warming signal
- Ensemble achieves **`r round(mae_ens, 1)`-day MAE** on rolling held-out years
- R and Python pipelines agree within **`r round(avg_gap, 1)` days** on average → blended submission

---

## Final 2026 Predictions {.smaller}

:::: {.columns}

::: {.column width="55%"}

```{r}
final_sub |>
  mutate(
    Date = format(doy_to_date(target_year, prediction), "%b %d"),
    `Interval` = paste0(
      format(doy_to_date(target_year, lower), "%b %d"), " – ",
      format(doy_to_date(target_year, upper), "%b %d")),
    Width = upper - lower
  ) |>
  rename(Location = location, DOY = prediction) |>
  select(Location, DOY, Date, Interval, Width) |>
  kable(align = "lrlrr")
```

<br>

**Sum of squared interval widths:** `r ssw`

| Metric | Value |
|---|---:|
| Ensemble backtest MAE | `r round(mae_ens, 2)` days |
| R vs Python mean gap | `r round(avg_gap, 1)` days |
| Local weight ($w_A$) | `r round(w_local * 100, 1)`% |
| GAM weight ($w_B$) | `r round(w_gam * 100, 1)`% |

:::

::: {.column width="45%"}

```{r}
#| fig-width: 5
#| fig-height: 4.5
final_sub |>
  mutate(date_pred  = doy_to_date(target_year, prediction),
         date_lower = doy_to_date(target_year, lower),
         date_upper = doy_to_date(target_year, upper),
         location   = reorder(location, prediction)) |>
  ggplot(aes(y = location)) +
  geom_segment(aes(x = date_lower, xend = date_upper, yend = location),
               linewidth = 4, color = "#F2A6B6", alpha = 0.7) +
  geom_point(aes(x = date_pred), size = 5, color = "#C0392B") +
  scale_x_date(date_labels = "%b %d") +
  labs(title = "2026 Predicted Bloom Dates",
       x = "Date", y = NULL) +
  theme_minimal(base_size = 12) +
  theme(panel.grid.major.y = element_blank())
```

:::

::::

---

## Thank You {.center}

<br>

### Cherry Blossom Peak Bloom Prediction 2026

**Team 5103 — University of Maryland**

<br>

::: {.callout-tip}
All code, data, and outputs are publicly available and fully reproducible.
:::

```bash
quarto render solution.qmd                              # R pipeline
jupyter nbconvert --execute Solution.ipynb --inplace     # Python pipeline
```
