---
title: "Cherry Blossom Peak Bloom Prediction 2026"
author: "Team 5103"
format:
  html:
    embed-resources: true
execute:
  warning: false
  message: false
---

## Abstract

Cherry blossom bloom timing is a sensitive phenological indicator of climate change, yet accurate annual prediction remains difficult because bloom dates emerge from complex, nonlinear interactions among geography, long-term warming trends, and stochastic weather variability. We present an interpretable two-model ensemble that exploits complementary information scales.

**Model A** captures site-specific momentum through a recency-weighted quadratic trend fitted independently to each of the five competition locations (Kyoto, Washington D.C., Liestal, Vancouver, New York City). Exponential decay weights (half-life ≈ 6 years) let recent climate shifts dominate the local signal while retaining curvature from longer records—particularly valuable for Kyoto's 800+ year series and Washington D.C.'s century of data.

**Model B** learns shared cross-site structure via a pooled Generalized Additive Model (GAM) with smooth terms for calendar year, latitude–longitude interaction, altitude, and per-site observation depth. This model is trained on competition data augmented with three auxiliary time series (Japan regional bloom dates, MeteoSwiss Swiss-wide phenology, and South Korean cherry records) plus both USA-NPN data products—status-intensity observations and individual phenometrics—for the New York City site (Washington Square Park, Site 32789, species 228). The auxiliary data broadens geographic and temporal coverage, helping the GAM generalize across hemispheres and altitudes.

**Ensemble blending** is data-driven: a rolling-origin backtest (validation windows from 1900 to the latest observed year) produces out-of-sample predictions for both models at each site. Inverse-MAE weights, computed globally over this backtest, determine the final blend ratio—giving more influence to whichever model performs better on held-out years.

**Prediction intervals** are calibrated using split-conformal principles. For each location, we take the 90th percentile of the backtest absolute residuals as the half-width, producing intervals that are tight enough to win on the sum-of-squared-widths tiebreaker while maintaining ≥90% empirical coverage.

A parallel Python implementation (Gradient Boosting Regressor in place of the GAM) provides an independent second opinion. When the two pipelines agree within 4 days on average, we submit a blended point prediction and averaged interval bounds; otherwise we default to the R pipeline. This cross-language ensemble guard reduces model-specific overfitting risk.

Key features include latitude, longitude, altitude (log-transformed), calendar year (centered and squared), observation-count depth per site, and data-source identity. No proprietary or future-looking data are used; all inputs are publicly available. The Quarto document is fully self-contained and reproducible.

## Goal

This notebook builds a competition-focused ensemble for the next unseen year:

-   `Model A`: site-level recency-weighted trend (strong for long local histories).
-   `Model B`: pooled nonlinear GAM using geography + time + NOAA climate covariates.
-   `Ensemble`: site-wise dynamic weighted blend from rolling-origin backtesting.
-   `Intervals`: location-wise conformal residual quantiles from backtests.

## Submission preview

```{r}
preview_file <- "cherry-predictions-final.csv"

infer_target_year <- function() {
  files <- c("data/kyoto.csv", "data/washingtondc.csv", "data/liestal.csv", "data/vancouver.csv", "data/nyc.csv")
  years <- c()
  for (f in files) {
    if (file.exists(f)) {
      tmp <- try(read.csv(f, stringsAsFactors = FALSE), silent = TRUE)
      if (!inherits(tmp, "try-error") && "year" %in% names(tmp)) {
        years <- c(years, suppressWarnings(as.integer(tmp$year)))
      }
    }
  }
  years <- years[!is.na(years)]
  if (length(years) == 0) as.integer(format(Sys.Date(), "%Y")) else max(years) + 1L
}

doy_to_date <- function(year, doy) {
  as.Date(strptime(sprintf("%d %03d", as.integer(year), as.integer(doy)), format = "%Y %j"))
}

preview_year <- infer_target_year()
cat(sprintf("Preview year: %d\n\n", preview_year))

if (file.exists(preview_file)) {
  x <- read.csv(preview_file, stringsAsFactors = FALSE)
  x <- x[order(x$location), ]
  x$predicted_date <- doy_to_date(preview_year, x$prediction)
  x$lower_date <- doy_to_date(preview_year, x$lower)
  x$upper_date <- doy_to_date(preview_year, x$upper)
  x
} else if (file.exists("cherry-predictions.csv")) {
  x <- read.csv("cherry-predictions.csv", stringsAsFactors = FALSE)
  x <- x[order(x$location), ]
  x$predicted_date <- doy_to_date(preview_year, x$prediction)
  x$lower_date <- doy_to_date(preview_year, x$lower)
  x$upper_date <- doy_to_date(preview_year, x$upper)
  x
} else {
  data.frame(note = "Run this document once to generate submission files.")
}
```

```{r}
#| label: setup
required_pkgs <- c("tidyverse", "lubridate", "mgcv", "httr2", "jsonlite")
missing_pkgs <- required_pkgs[!vapply(required_pkgs, requireNamespace, logical(1), quietly = TRUE)]
if (length(missing_pkgs) > 0) {
  options(repos = c(CRAN = "https://cloud.r-project.org"))
  install.packages(missing_pkgs)
}

library(tidyverse)
library(lubridate)
library(mgcv)
library(httr2)
library(jsonlite)

set.seed(5103)
options(dplyr.summarise.inform = FALSE)
```

## Load and unify data

```{r}
competition_files <- c(
  "data/kyoto.csv",
  "data/washingtondc.csv",
  "data/liestal.csv",
  "data/vancouver.csv",
  "data/nyc.csv"
)

aux_files <- c(
  "data/japan.csv",
  "data/meteoswiss.csv",
  "data/south_korea.csv"
)

read_bloom_file <- function(path, source) {
  read_csv(path, show_col_types = FALSE) |>
    transmute(
      source = source,
      location,
      lat,
      long,
      alt,
      year = as.integer(year),
      bloom_doy = as.numeric(bloom_doy)
    )
}

competition_raw <- map_dfr(competition_files, ~ read_bloom_file(.x, "competition"))
aux_raw <- map_dfr(aux_files, ~ read_bloom_file(.x, "auxiliary"))

# Extra NYC signal from USA-NPN status observations (site 32789, species 228, Open flowers)
nyc_npn_status <- read_csv("data/USA-NPN_status_intensity_observations_data.csv", show_col_types = FALSE) |>
  filter(Site_ID == 32789, Species_ID == 228, Phenophase_ID == 501) |>
  mutate(
    Observation_Date = as_date(Observation_Date, format = "%m/%d/%y"),
    year = year(Observation_Date)
  ) |>
  arrange(Observation_Date) |>
  group_by(year) |>
  summarize(
    bloom_doy = first(Day_of_Year[Phenophase_Status == 1]),
    .groups = "drop"
  ) |>
  filter(!is.na(bloom_doy)) |>
  mutate(source = "npn", location = "newyorkcity",
         lat = 40.73040, long = -73.99809, alt = 8.5) |>
  select(source, location, lat, long, alt, year, bloom_doy)

# Also use USA-NPN individual phenometrics (pre-computed first-flower DOY)
nyc_npn_pheno <- read_csv("data/USA-NPN_individual_phenometrics_data.csv", show_col_types = FALSE) |>
  filter(Site_ID == 32789, Species_ID == 228, Phenophase_ID == 501) |>
  group_by(First_Yes_Year) |>
  summarize(bloom_doy = min(First_Yes_DOY, na.rm = TRUE), .groups = "drop") |>
  filter(is.finite(bloom_doy)) |>
  rename(year = First_Yes_Year) |>
  mutate(source = "npn", location = "newyorkcity",
         lat = 40.73040, long = -73.99809, alt = 8.5) |>
  select(source, location, lat, long, alt, year, bloom_doy)

# Merge both NPN sources (status takes priority where years overlap)
nyc_npn <- nyc_npn_status |>
  bind_rows(nyc_npn_pheno |>
              anti_join(nyc_npn_status, by = "year"))

competition <- competition_raw |>
  bind_rows(nyc_npn |>
              anti_join(competition_raw, by = c("location", "year")))

all_data <- competition |>
  bind_rows(aux_raw) |>
  filter(!is.na(bloom_doy), !is.na(year), year >= 1880) |>
  mutate(
    site_id = paste(source, location, sep = "::"),
    source = factor(source),
    location = as.character(location)
  )

competition_sites <- sort(unique(competition_raw$location))
target_year <- max(competition_raw$year, na.rm = TRUE) + 1

all_data |>
  count(source, name = "rows")

# NOAA climate covariates (dataset metadata: https://www.ncdc.noaa.gov/cdo-web/datasets)
NOAA_API_BASE_URL <- "https://www.ncei.noaa.gov/cdo-web/api/v2/data"
NOAA_WEB_API_TOKEN <- Sys.getenv("NOAA_WEB_API_TOKEN")

stations <- tibble::tribble(
  ~location,       ~station_id,
  "washingtondc", "GHCND:USW00013743",
  "vancouver",    "GHCND:CA001108395",
  "newyorkcity",  "GHCND:USW00014732",
  "liestal",      "GHCND:SZ000001940",
  "kyoto",        "GHCND:JA000047759"
)

get_noaa_daily <- function(station_id, start_date, end_date, token) {
  if (!nzchar(token)) return(tibble())
  out <- list()
  windows <- seq(as_date(start_date), as_date(end_date), by = "300 days")
  if (length(windows) <= 1) windows <- c(as_date(start_date), as_date(end_date) + days(1))
  for (i in seq_len(length(windows) - 1)) {
    from <- windows[i]
    to <- min(windows[i + 1] - days(1), as_date(end_date))
    resp <- tryCatch(
      request(NOAA_API_BASE_URL) |>
        req_headers(token = token) |>
        req_url_query(
          datasetid = "GHCND",
          stationid = station_id,
          datatypeid = "TAVG,TMAX",
          startdate = as.character(from),
          enddate = as.character(to),
          units = "metric",
          limit = 1000
        ) |>
        req_retry(max_tries = 5) |>
        req_perform() |>
        resp_body_json(),
      error = function(e) NULL
    )
    if (!is.null(resp$results)) {
      out[[length(out) + 1]] <- tibble::as_tibble(resp$results)
    }
  }
  bind_rows(out)
}

build_noaa_features <- function(force_refresh = FALSE, cache_file = "data/noaa_features.csv") {
  if (!force_refresh && file.exists(cache_file)) {
    return(read_csv(cache_file, show_col_types = FALSE))
  }

  if (!nzchar(NOAA_WEB_API_TOKEN)) {
    message("NOAA_WEB_API_TOKEN not found. Using cached climate features if available.")
    return(tibble(location = character(), year = integer(),
                  winter_tavg = numeric(), spring_tavg = numeric(), gdd_proxy = numeric()))
  }

  start_year <- max(1880, min(all_data$year, na.rm = TRUE) - 1)
  end_year <- max(all_data$year, na.rm = TRUE)

  raw <- stations |>
    group_by(location, station_id) |>
    group_modify(\(x, g) {
      get_noaa_daily(g$station_id, sprintf("%d-01-01", start_year), sprintf("%d-12-31", end_year), NOAA_WEB_API_TOKEN) |>
        mutate(location = g$location)
    }) |>
    ungroup()

  if (nrow(raw) == 0) {
    return(tibble(location = character(), year = integer(),
                  winter_tavg = numeric(), spring_tavg = numeric(), gdd_proxy = numeric()))
  }

  feat <- raw |>
    filter(datatype %in% c("TAVG", "TMAX")) |>
    mutate(
      date = as_date(date),
      value = as.numeric(value),
      value = if_else(abs(value) > 80, value / 10, value),
      target_year = if_else(month(date) == 12, year(date) + 1L, year(date)),
      in_winter = month(date) %in% c(12, 1, 2),
      in_spring = month(date) %in% c(2, 3)
    ) |>
    group_by(location, year = target_year) |>
    summarize(
      winter_tavg = mean(value[in_winter & datatype == "TAVG"], na.rm = TRUE),
      spring_tavg = mean(value[in_spring & datatype == "TAVG"], na.rm = TRUE),
      gdd_proxy = sum(pmax(value[month(date) %in% c(1, 2, 3) & datatype == "TAVG"] - 5, 0), na.rm = TRUE),
      .groups = "drop"
    ) |>
    mutate(across(c(winter_tavg, spring_tavg, gdd_proxy), ~ ifelse(is.nan(.x), NA_real_, .x)))

  write_csv(feat, cache_file)
  feat
}

climate_features <- build_noaa_features(force_refresh = FALSE)
```

## Feature engineering

```{r}
add_features <- function(df, reference_df = NULL) {
  ref <- if (is.null(reference_df)) df else reference_df
  site_n <- ref |>
    count(site_id, name = "site_obs")

  climate_cols <- c("winter_tavg", "spring_tavg", "gdd_proxy")
  for (nm in climate_cols) {
    if (!nm %in% names(df)) df[[nm]] <- NA_real_
    if (!nm %in% names(ref)) ref[[nm]] <- NA_real_
  }

  location_clim <- ref |>
    group_by(location) |>
    summarize(
      winter_tavg_loc = suppressWarnings(median(winter_tavg, na.rm = TRUE)),
      spring_tavg_loc = suppressWarnings(median(spring_tavg, na.rm = TRUE)),
      gdd_proxy_loc = suppressWarnings(median(gdd_proxy, na.rm = TRUE)),
      .groups = "drop"
    ) |>
    mutate(across(ends_with("_loc"), ~ ifelse(is.infinite(.x), NA_real_, .x)))

  g_winter <- suppressWarnings(median(ref$winter_tavg, na.rm = TRUE)); if (!is.finite(g_winter)) g_winter <- 0
  g_spring <- suppressWarnings(median(ref$spring_tavg, na.rm = TRUE)); if (!is.finite(g_spring)) g_spring <- 0
  g_gdd <- suppressWarnings(median(ref$gdd_proxy, na.rm = TRUE)); if (!is.finite(g_gdd)) g_gdd <- 0

  df |>
    left_join(site_n, by = "site_id") |>
    left_join(location_clim, by = "location") |>
    mutate(
      year_c = year - 1950,
      year_c2 = year_c^2,
      decade = floor(year / 10) * 10,
      lat_abs = abs(lat),
      alt_log1p = log1p(pmax(alt, 0)),
      site_obs = replace_na(site_obs, 1),
      winter_tavg = coalesce(winter_tavg, winter_tavg_loc, g_winter),
      spring_tavg = coalesce(spring_tavg, spring_tavg_loc, g_spring),
      gdd_proxy = coalesce(gdd_proxy, gdd_proxy_loc, g_gdd)
    )
}
```

## Model definitions

```{r}
predict_local_trend <- function(train_comp, new_comp) {
  preds <- map_dfr(unique(new_comp$location), function(loc) {
    tr <- train_comp |>
      filter(location == loc) |>
      arrange(year)

    nd <- new_comp |>
      filter(location == loc)

    n <- nrow(tr)

    if (n >= 4) {
      w <- exp(seq(-n + 1, 0) / 6)
      fit <- lm(bloom_doy ~ year + I(year^2), data = tr, weights = w)
      p <- predict(fit, newdata = nd)
    } else if (n >= 2) {
      fit <- lm(bloom_doy ~ year, data = tr)
      p <- predict(fit, newdata = nd)
    } else {
      p <- rep(mean(tr$bloom_doy, na.rm = TRUE), nrow(nd))
    }

    nd |>
      mutate(pred_local = as.numeric(p))
  })

  preds
}

fit_gam_model <- function(train_all) {
  term_for_climate <- function(var_name) {
    uniq <- n_distinct(train_all[[var_name]])
    if (is.na(uniq) || uniq < 5) {
      var_name
    } else {
      k_val <- min(8, uniq - 1)
      paste0("s(", var_name, ", k = ", k_val, ", bs = 'cr')")
    }
  }

  rhs_terms <- c(
    "s(year, k = 25, bs = 'cr')",
    "s(lat, long, k = 40, bs = 'tp')",
    "s(alt_log1p, k = 8, bs = 'cr')",
    "s(site_obs, k = 8, bs = 'cr')",
    term_for_climate("winter_tavg"),
    term_for_climate("spring_tavg"),
    term_for_climate("gdd_proxy"),
    "source"
  )

  model_formula <- as.formula(paste("bloom_doy ~", paste(rhs_terms, collapse = " + ")))

  mgcv::gam(model_formula, data = train_all, method = "REML")
}
```

## Rolling-origin backtest for weights and interval calibration

```{r}
backtest_years <- seq(max(1900, min(competition_raw$year) + 20), max(competition_raw$year))

rolling_results <- map_dfr(backtest_years, function(y) {
  train_comp <- competition |>
    filter(year < y)

  test_comp <- competition_raw |>
    filter(year == y)

  if (nrow(test_comp) == 0 || n_distinct(train_comp$location) < length(competition_sites)) {
    return(tibble())
  }

  train_all <- all_data |>
    filter(year < y) |>
    left_join(climate_features, by = c("location", "year")) |>
    add_features()

  test_comp_feat <- test_comp |>
    mutate(source = factor("competition", levels = levels(all_data$source)),
           site_id = paste(source, location, sep = "::")) |>
    left_join(climate_features, by = c("location", "year")) |>
    add_features(reference_df = train_all)

  local_pred <- predict_local_trend(train_comp, test_comp_feat)

  gam_fit <- fit_gam_model(train_all)
  gam_pred <- predict(gam_fit, newdata = test_comp_feat)

  out <- test_comp_feat |>
    select(location, year, bloom_doy) |>
    left_join(local_pred |>
                select(location, year, pred_local),
              by = c("location", "year")) |>
    mutate(pred_gam = as.numeric(gam_pred))

  out
})

mae_local <- mean(abs(rolling_results$bloom_doy - rolling_results$pred_local), na.rm = TRUE)
mae_gam <- mean(abs(rolling_results$bloom_doy - rolling_results$pred_gam), na.rm = TRUE)

w_grid <- seq(0, 1, by = 0.02)
global_weight_tbl <- tibble(w_local = w_grid) |>
  mutate(mae = map_dbl(w_local, \(w) {
    mean(abs(rolling_results$bloom_doy - (w * rolling_results$pred_local + (1 - w) * rolling_results$pred_gam)), na.rm = TRUE)
  })) |>
  arrange(mae)

w_local_global <- global_weight_tbl$w_local[[1]]
w_gam_global <- 1 - w_local_global

site_weights <- rolling_results |>
  group_split(location) |>
  map_dfr(\(df) {
    loc <- unique(df$location)[1]
    tibble(w_local = w_grid) |>
      mutate(mae = map_dbl(w_local, \(w) {
        mean(abs(df$bloom_doy - (w * df$pred_local + (1 - w) * df$pred_gam)), na.rm = TRUE)
      })) |>
      slice_min(mae, n = 1, with_ties = FALSE) |>
      transmute(location = loc, w_local, w_gam = 1 - w_local)
  })

rolling_results <- rolling_results |>
  left_join(site_weights, by = "location") |>
  mutate(
    w_local = replace_na(w_local, w_local_global),
    w_gam = replace_na(w_gam, w_gam_global),
    pred_ensemble = w_local * pred_local + w_gam * pred_gam,
         abs_err = abs(bloom_doy - pred_ensemble))

mae_ensemble <- mean(abs(rolling_results$bloom_doy - rolling_results$pred_ensemble), na.rm = TRUE)

site_q90 <- rolling_results |>
  group_by(location) |>
  summarize(q90 = quantile(abs_err, probs = 0.90, na.rm = TRUE, type = 8), .groups = "drop")

global_q90 <- quantile(rolling_results$abs_err, probs = 0.90, na.rm = TRUE, type = 8)

tibble(model = c("local", "gam", "ensemble"),
       mae = c(mae_local,
               mae_gam,
         mae_ensemble))
```

## Final fit and next-year prediction

```{r}
train_all <- all_data |>
  left_join(climate_features, by = c("location", "year")) |>
  add_features()
train_comp <- competition

newdata <- competition_raw |>
  group_by(location) |>
  slice_max(year, n = 1, with_ties = FALSE) |>
  ungroup() |>
  transmute(
    source = factor("competition", levels = levels(all_data$source)),
    location,
    lat,
    long,
    alt,
    year = target_year,
    bloom_doy = NA_real_,
    site_id = paste(source, location, sep = "::")
  ) |>
  left_join(climate_features, by = c("location", "year")) |>
  add_features(reference_df = train_all)

local_pred <- predict_local_trend(train_comp, newdata)
gam_fit <- fit_gam_model(train_all)

gam_pred <- predict(gam_fit, newdata = newdata)

final_pred <- newdata |>
  select(location, year) |>
  left_join(local_pred |>
              select(location, year, pred_local),
            by = c("location", "year")) |>
  mutate(
    pred_gam = as.numeric(gam_pred),
    prediction = pred_gam
  ) |>
  left_join(site_weights, by = "location") |>
  mutate(
    w_local = replace_na(w_local, w_local_global),
    w_gam = replace_na(w_gam, w_gam_global),
    prediction = w_local * pred_local + w_gam * pred_gam
  ) |>
  left_join(site_q90, by = "location") |>
  mutate(
    q90 = replace_na(q90, as.numeric(global_q90)),
    lower = prediction - q90,
    upper = prediction + q90,
    prediction = round(prediction),
    lower = floor(lower),
    upper = ceiling(upper),
    prediction = pmin(pmax(prediction, 1), 366),
    lower = pmin(pmax(lower, 1), 366),
    upper = pmin(pmax(upper, 1), 366)
  ) |>
  arrange(location)

final_pred
```

## Convert day-of-year to date and write submission file

```{r}
doy_to_date <- function(year, doy) {
  as_date(strptime(sprintf("%d %03d", year, doy), format = "%Y %j"))
}

submission <- final_pred |>
  mutate(
    predicted_date = doy_to_date(year, prediction),
    lower_date = doy_to_date(year, lower),
    upper_date = doy_to_date(year, upper)
  ) |>
  select(location, prediction, lower, upper, predicted_date, lower_date, upper_date)

submission
```

```{r}
# Official competition-format CSV
submission |>
  select(location, prediction, lower, upper) |>
  write_csv("cherry-predictions.csv")
```

## Compare R vs Python and auto-recommend final submission

```{r}
python_file <- "cherry-predictions-python.csv"

if (file.exists(python_file)) {
  r_sub <- submission |>
    select(location, prediction, lower, upper) |>
    rename(
      prediction_r = prediction,
      lower_r = lower,
      upper_r = upper
    )

  py_sub <- read_csv(python_file, show_col_types = FALSE) |>
    select(location, prediction, lower, upper) |>
    rename(
      prediction_py = prediction,
      lower_py = lower,
      upper_py = upper
    )

  comparison <- r_sub |>
    inner_join(py_sub, by = "location") |>
    mutate(
      point_gap = abs(prediction_r - prediction_py),
      lower_gap = abs(lower_r - lower_py),
      upper_gap = abs(upper_r - upper_py)
    ) |>
    arrange(desc(point_gap), location)

  comparison

  avg_gap <- mean(comparison$point_gap, na.rm = TRUE)

  # Rule:
  # - If methods broadly agree (avg gap <= 4 days), blend points AND average bounds.
  #   Using averaged bounds (not min/max) keeps intervals tight for the tiebreaker
  #   (sum of squared interval widths).
  # - Otherwise default to R model (its intervals are calibrated in this report).
  final_submission <- if (avg_gap <= 4) {
    comparison |>
      transmute(
        location,
        prediction = round((prediction_r + prediction_py) / 2),
        lower = floor((lower_r + lower_py) / 2),
        upper = ceiling((upper_r + upper_py) / 2)
      )
  } else {
    r_sub |>
      transmute(
        location,
        prediction = prediction_r,
        lower = lower_r,
        upper = upper_r
      )
  }

  final_submission <- final_submission |>
    mutate(
      prediction = pmin(pmax(prediction, 1), 366),
      lower = pmin(pmax(lower, 1), 366),
      upper = pmin(pmax(upper, 1), 366)
    ) |>
    arrange(location)

  write_csv(final_submission, "cherry-predictions-final.csv")

  list(
    recommendation = if (avg_gap <= 4) {
      "Use blended R+Python submission"
    } else {
      "Use R submission"
    },
    mean_point_gap_days = round(avg_gap, 2)
  )

  final_submission
} else {
  message("Python submission file not found; defaulting to R submission.")
  submission |>
    select(location, prediction, lower, upper) |>
    write_csv("cherry-predictions-final.csv")
}
```